{
 "cells": [
  {
   "source": [
    "# 4442 Group Project #\n",
    "## Stock Market Forecasting 5 Major Blue Chip Stocks Using Time Series Analysis ##\n",
    "** Ian Borwick ** <br>\n",
    "** Lucas Fraulin 250963527 ** <br>\n",
    "** Benjamin Tilden 250959344 ** <br>"
   ],
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY6Z7FUe-hPp"
   }
  },
  {
   "source": [
    "### What is the stock market? ###\n",
    "\n",
    "Add a description here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is ARIMA ###\n",
    "\n",
    "Add a description here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
    "                        FutureWarning)\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
    "                        FutureWarning)\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from pmdarima.arima import auto_arima\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "source": [
    "Create Stock class to hold stock data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a date parser\n",
    "dateParser = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d')\n",
    "\n",
    "class Stock:\n",
    "    def __init__(self, name, filename):\n",
    "        self.name = name\n",
    "        self.data = pd.read_csv(filename, sep=',', index_col='Date', parse_dates=['Date'], date_parser=dateParser).fillna(0)\n",
    "        self.df_log = np.log(self.data.Close)\n",
    "        self.train_data, self.test_data = self.df_log[3:int(len(self.df_log)*0.9)], self.df_log[int(len(self.df_log)*0.9):]\n",
    "\n",
    "    def getName(self):\n",
    "        return self.name\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "\n",
    "    def getDf_log(self):\n",
    "        return self.df_log\n",
    "\n",
    "    def getTrain(self):\n",
    "        return self.train_data\n",
    "    \n",
    "    def getTest(self):\n",
    "        return self.test_data"
   ]
  },
  {
   "source": [
    "Initialize Stock Data Objects"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = Stock(\"Apple: AAPL\", \"aapl.us.txt\")\n",
    "google = Stock(\"Google: GOOGL\", \"googl.us.txt\")\n",
    "amazon = Stock(\"Amazon: AMZN\", \"amzn.us.txt\")\n",
    "alibaba = Stock(\"Alibaba: BABA\", \"baba.us.txt\")\n",
    "berkshireHathaway = Stock(\"Berkshire Hathaway: BRK A\", \"brk-a.us.txt\")\n",
    "facebook = Stock(\"Facebook: FB\", \"fb.us.txt\")\n",
    "jpmorgan = Stock(\"JPMorgan: JPM\", \"jpm.us.txt\")\n",
    "microsoft = Stock(\"Microsoft: MCFT\", \"mcft.us.txt\")\n",
    "tesla = Stock(\"Tesla: TSLA\", \"tsla.us.txt\")\n",
    "visa = Stock(\"Visa: V\", \"v.us.txt\")"
   ]
  },
  {
   "source": [
    "Plot of CLosing Prices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClosingPrice(data, name):\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.grid(True)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Close Prices')\n",
    "        plt.plot(data['Close'])\n",
    "        plt.title(name + '  Closing Price')\n",
    "        plt.show()"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "Kernel Density Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotKernalDensityEstimate(data, name):\r\n",
    "        data.Close.plot(kind='kde')\r\n",
    "        plt.title(name + \"  Kernal Density Estimate\")\r\n",
    "        plt.show() #this line can be taken out to show all the plots against each other "
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "Also, a given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise.\n",
    "\n",
    "These components are defined as follows:\n",
    "\n",
    "    Level: The average value in the series.\n",
    "\n",
    "    Trend: The increasing or decreasing value in the series.\n",
    "\n",
    "    Seasonality: The repeating short-term cycle in the series.\n",
    "\n",
    "    Noise: The random variation in the series.\n",
    "\n",
    "First, we need to check if a series is stationary or not because time series analysis only works with stationary data.\n",
    "\n",
    "\n",
    "<h3 style=\"color: skyblue; padding-top: 10px; text-decoration: underline;\">ADF (Augmented Dickey-Fuller) Test</h3></red>\n",
    "\n",
    "The Dickey-Fuller test is one of the most popular statistical tests. It can be used to determine the presence of unit root in the series, and hence help us understand if the series is stationary or not. The null and alternate hypothesis of this test is:\n",
    "\n",
    "Null Hypothesis: The series has a unit root (value of a =1)\n",
    "\n",
    "Alternate Hypothesis: The series has no unit root.\n",
    "\n",
    "If we fail to reject the null hypothesis, we can say that the series is non-stationary. This means that the series can be linear or difference stationary.\n",
    "\n",
    "If both mean and standard deviation are flat lines(constant mean and constant variance), the series becomes stationary.\n",
    "\n",
    "So let's check for stationarity:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for staionarity\n",
    "def test_stationarity(timeseries, name):\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "    #Plot rolling statistics:\n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(name + '  Rolling Mean and Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    print(\"Results of dickey fuller test\")\n",
    "    adft = adfuller(timeseries,autolag='AIC')\n",
    "    # output for dft will give us without defining what the values are.\n",
    "    #hence we manually write what values does it explains using a for loop\n",
    "    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n",
    "    for key,values in adft[4].items():\n",
    "        output['critical value (%s)'%key] =  values\n",
    "    print(output)"
   ]
  },
  {
   "source": [
    "Through the above graph, we can see the increasing mean and standard deviation and hence our series is not stationary.\n",
    "\n",
    "We see that the p-value is greater than 0.05 so we cannot reject the Null hypothesis. Also, the test statistics is greater than the critical values. so the data is non-stationary.\n",
    "\n",
    "In order to perform a time series analysis, we may need to separate seasonality and trend from our series. The resultant series will become stationary through this process.\n",
    "\n",
    "So let us separate Trend and Seasonality from the time series."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To separate the trend and the seasonality from a time series, \n",
    "# we can decompose the series using the following code.\n",
    "\n",
    "#seperate trend and seasonality by decomposing\n",
    "def seasonalDecompose(data, name):\n",
    "    result = seasonal_decompose(data.Close, model='multiplicative', freq = 30)\n",
    "    fig = plt.figure()\n",
    "    fig = result.plot()\n",
    "    fig.suptitle(name, fontsize=20) \n",
    "    fig.set_size_inches(16, 9)"
   ]
  },
  {
   "source": [
    "We start by taking a log of the series to reduce the magnitude of the values and reduce the rising trend in the series. Then after getting the log of the series, we find the rolling average of the series. A rolling average is calculated by taking input for the past 12 months and giving a mean consumption value at every point further ahead in series."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not stationary then eliminate trend\n",
    "#eliminate trend\n",
    "def movingAverage(data, name):\n",
    "    df_log = np.log(data.Close)\n",
    "    moving_avg = df_log.rolling(12).mean()\n",
    "    std_dev = df_log.rolling(12).std()\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(name + ': Moving Average')\n",
    "    plt.plot(std_dev, color =\"black\", label = \"Standard Deviation\")\n",
    "    plt.plot(moving_avg, color=\"red\", label = \"Mean\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "source": [
    "<h3 style=\"color: skyblue;\">Now we are going to create an ARIMA model and will train it with the closing price of the stock on the train data. So let us split the data into training and test set and visualize it.</h3>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestClosingPrices(data, name):\n",
    "    df_log = np.log(data.Close)\n",
    "    train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Closing Prices')\n",
    "    plt.title(name)\n",
    "    plt.plot(df_log, 'green', label='Train data')\n",
    "    plt.plot(test_data, 'blue', label='Test data')\n",
    "    plt.legend()"
   ]
  },
  {
   "source": [
    "Its time to choose parameters p,q,d for ARIMA model. Last time we chose the value of p,d, and q by observing the plots of ACF and PACF but now we are going to use Auto ARIMA to get the best parameters without even plotting ACF and PACF graphs.\n",
    "\n",
    "<i>Auto ARIMA: Automatically discover the optimal order for an ARIMA model. The auto_arima function seeks to identify the most optimal parameters for an ARIMA model, and returns a fitted ARIMA model. This function is based on the commonly-used R function, forecast::auto.arima.</i>\n",
    "\n",
    "The auro_arima function works by conducting differencing tests (i.e., Kwiatkowski–Phillips–Schmidt–Shin, Augmented Dickey-Fuller or Phillips–Perron) to determine the order of differencing, d, and then fitting models within ranges of defined start_p, max_p, start_q, max_q ranges. If the seasonal optional is enabled, auto_arima also seeks to identify the optimal P and Q hyper- parameters after conducting the Canova-Hansen to determine the optimal order of seasonal differencing, D."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoArima(data, name):\n",
    "    df_log = np.log(data.Close)\n",
    "    train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n",
    "    print(\"\\n\\n\\n\" + name + \" Auto ARIMA \")\n",
    "    model_autoARIMA = auto_arima(train_data, start_p=0, start_q=0,\n",
    "                    test='adf',       # use adftest to find optimal 'd'\n",
    "                    max_p=3, max_q=3, # maximum p and q\n",
    "                    m=1,              # frequency of series\n",
    "                    d=None,           # let model determine 'd'\n",
    "                    seasonal=False,   # No Seasonality\n",
    "                    start_P=0, \n",
    "                    D=0, \n",
    "                    trace=True,\n",
    "                    error_action='ignore',  \n",
    "                    suppress_warnings=True, \n",
    "                    stepwise=True)\n",
    "    print(model_autoARIMA.summary())\n",
    "    model_autoARIMA.plot_diagnostics(figsize=(15,8))\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "So how to interpret the plot diagnostics?\n",
    "\n",
    "    Top left: The residual errors seem to fluctuate around a mean of zero and have a uniform variance.\n",
    "\n",
    "    Top Right: The density plot suggest normal distribution with mean zero.\n",
    "\n",
    "    Bottom left: All the dots should fall perfectly in line with the red line. Any significant deviations would imply the distribution is skewed.\n",
    "\n",
    "    Bottom Right: The Correlogram, aka, ACF plot shows the residual errors are not autocorrelated. Any autocorrelation would imply that there is some pattern in the residual errors which are not explained in the model. So you will need to look for more X’s (predictors) to the model.\n",
    "    Overall, it seems to be a good fit. Let’s start forecasting the stock prices.\n",
    "\n",
    "Next, create an ARIMA model with provided optimal parameters p, d and q."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "def buildArimaModel(data, name, printmodel):\n",
    "    df_log = np.log(data.Close)\n",
    "    train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n",
    "    model = ARIMA(train_data, order=(1,1,2))  \n",
    "    fitted = model.fit(disp=-1)  \n",
    "    if printmodel == True:\n",
    "        print(fitted.summary())\n",
    "    return fitted"
   ]
  },
  {
   "source": [
    "Now let's start forecast the stock prices on the test dataset keeping 95% confidence level."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "def forecast(data, name):\n",
    "    df_log = np.log(data.Close)\n",
    "    train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n",
    "    fitted = buildArimaModel(data, name, False)\n",
    "    fc, se, conf = fitted.forecast(len(test_data.index), alpha=0.05)  # 95% conf\n",
    "    fc_series = pd.Series(fc, index=test_data.index)\n",
    "    lower_series = pd.Series(conf[:, 0], index=test_data.index)\n",
    "    upper_series = pd.Series(conf[:, 1], index=test_data.index)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10,5), dpi=100)\n",
    "    plt.plot(train_data, label='training data')\n",
    "    plt.plot(test_data, color = 'blue', label='Actual Stock Price')\n",
    "    plt.plot(fc_series, color = 'orange',label='Predicted Stock Price')\n",
    "    plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                    color='k', alpha=.10)\n",
    "    plt.title(name + ' Stock Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(name + ' Stock Price')\n",
    "    plt.legend(loc='upper left', fontsize=8)\n",
    "    plt.show()\n"
   ]
  },
  {
   "source": [
    "Mertics Meaning\n",
    "\n",
    "    diff_pred: difference between the predicted current point and one before it\n",
    "    diff_true: difference between true current point and one before it\n",
    "    sign_pred: positive or negative sign of the difference for prediction\n",
    "    sign_true: positive or negative sign of the difference for true value\n",
    "    is_correct: 1 if sign_pred == sign_true, otherwise 0\n",
    "    is_incorrect: 0 if sign_pred == sign_true, otherwise 1\n",
    "    is_predicted: 1 if the model has made a valid prediction 0 if not (used for confidence levels)\n",
    "    result: the loss resulting from the predicted sign applied to the actual value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe thats useful for metrics\n",
    "def makedf(data, name):\n",
    "    df_log = np.log(data.Close)\n",
    "    train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n",
    "    fitted = buildArimaModel(data, name, False)\n",
    "    fc, se, conf = fitted.forecast(len(test_data.index), alpha=0.05)  # 95% conf\n",
    "    y_true = pd.DataFrame(pd.Series(fc, index=test_data.index))\n",
    "    y_pred = pd.DataFrame(test_data)\n",
    "    \n",
    "    df = pd.concat([y_pred, y_true],axis=1)\n",
    "    df.columns = ['y_pred', 'y_true']\n",
    "\n",
    "    df['diff_pred'] = df.y_pred.diff()\n",
    "    df['diff_true'] = df.y_true.diff()\n",
    "    df['sign_pred'] = df.diff_pred.apply(np.sign)\n",
    "    df['sign_true'] = df.diff_true.apply(np.sign)\n",
    "    df['is_correct'] = 0\n",
    "    df.loc[df.sign_pred *df.sign_true >0, 'is_correct'] = 1\n",
    "    df['is_incorrect'] = 0\n",
    "    df.loc[df.sign_pred *df.sign_true <0, 'is_incorrect'] = 1\n",
    "    df['is_predicted'] = df.is_correct +df.is_incorrect\n",
    "    df['result'] = df.sign_pred * df.y_true\n",
    "    return df\n",
    "\n",
    "#makeDF from ypred, ytrue (for use in compare models function)\n",
    "def makeDF(yPred, yTrue, name):\n",
    "    y_true = pd.DataFrame(yTrue)\n",
    "    y_pred = pd.DataFrame(yPred)\n",
    "    \n",
    "    df = pd.concat([y_pred, y_true],axis=1)\n",
    "    df.columns = ['y_pred', 'y_true']\n",
    "\n",
    "    df['diff_pred'] = df.y_pred.diff()\n",
    "    df['diff_true'] = df.y_true.diff()\n",
    "    df['sign_pred'] = df.diff_pred.apply(np.sign)\n",
    "    df['sign_true'] = df.diff_true.apply(np.sign)\n",
    "    df['is_correct'] = 0\n",
    "    df.loc[df.sign_pred *df.sign_true >0, 'is_correct'] = 1\n",
    "    df['is_incorrect'] = 0\n",
    "    df.loc[df.sign_pred *df.sign_true <0, 'is_incorrect'] = 1\n",
    "    df['is_predicted'] = df.is_correct +df.is_incorrect\n",
    "    df['result'] = df.sign_pred * df.y_true\n",
    "    return df"
   ]
  },
  {
   "source": [
    "calc scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScores(df):\n",
    "    scores = pd.Series()\n",
    "    \n",
    "    #building block metrics\n",
    "    scores.loc['accuracy'] = df.is_correct.sum()*1. / (df.is_predicted.sum()*1.)*100\n",
    "    scores.loc['edge'] = df.result.mean()\n",
    "    scores.loc['noise'] = df.y_pred.diff().abs().mean()\n",
    "    \n",
    "    #metrics\n",
    "    scores.loc['yTrueChange'] = df.y_true.abs().mean()\n",
    "    scores.loc['yPredChange'] = df.y_pred.abs().mean()\n",
    "    scores.loc['predictionCalibration'] = scores.loc['yPredChange']/scores.loc['yTrueChange']\n",
    "    scores.loc['captureRatio'] = scores.loc['edge']/scores.loc['yTrueChange']*100\n",
    "    \n",
    "    #subset prediction metrics\n",
    "    scores.loc['edgeLong'] = df[df.sign_pred == 1].result.mean()-df.y_true.mean()\n",
    "    scores.loc['edgeShort'] = df[df.sign_pred == -1].result.mean()-df.y_true.mean()\n",
    "    scores.loc['edgeWin'] = df[df.is_correct ==1].result.mean() - df.y_true.mean()\n",
    "    scores.loc['edgeLose'] = df[df.is_incorrect ==1].result.mean() - df.y_true.mean()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "source": [
    "scores by year"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoresByYear(df):\n",
    "        df['year'] = df.index.get_level_values('Date').year\n",
    "        return df.groupby('year').apply(calcScores).T"
   ]
  },
  {
   "source": [
    "comparing different models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing to different models\n",
    "\n",
    "def compareModels(data, train_data, test_data, name):\n",
    "    fitted = buildArimaModel(data, name, False)\n",
    "    fc, se, conf = fitted.forecast(len(test_data.index), alpha=0.05)  # 95% conf\n",
    "    fc_series = pd.Series(fc, index=test_data.index)\n",
    "    #preprocess and split data\n",
    "    xTrain = np.array(train_data.index.map(datetime.datetime.toordinal)).reshape(-1,1)\n",
    "    xTest = np.array(test_data.index.map(datetime.datetime.toordinal)).reshape(-1,1)\n",
    "    yTrain = train_data.values\n",
    "    yTest = test_data.values\n",
    "\n",
    "    #linear regression\n",
    "    lr = LinearRegression().fit(xTrain,yTrain)\n",
    "    lr_train = pd.Series(lr.predict(xTrain))\n",
    "    lr_test = pd.Series(lr.predict(xTest))\n",
    "\n",
    "    #KNN \n",
    "    knn = KNeighborsRegressor(n_neighbors=80, weights='uniform', algorithm='auto', leaf_size=30, p=2).fit(xTrain,yTrain)\n",
    "    knn_train = pd.Series(knn.predict(xTrain))\n",
    "    knn_test = pd.Series(knn.predict(xTest))\n",
    "\n",
    "    #Lasso Regression \n",
    "    ls = Lasso(alpha =0.0001, max_iter=200000).fit(xTrain,yTrain)\n",
    "    ls_train = pd.Series(ls.predict(xTrain))\n",
    "    ls_test = pd.Series(ls.predict(xTest))\n",
    "\n",
    "    #Elastic Net\n",
    "    en = ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=20000).fit(xTrain,yTrain)\n",
    "    en_train = pd.Series(en.predict(xTrain))\n",
    "    en_test = pd.Series(en.predict(xTest))\n",
    "\n",
    "    #Ridge Regression\n",
    "    rr = Ridge(alpha=1).fit(xTrain,yTrain)\n",
    "    rr_train = pd.Series(rr.predict(xTrain))\n",
    "    rr_test = pd.Series(rr.predict(xTest))\n",
    "\n",
    "    #Random Forest Regressor\n",
    "    rf = RandomForestRegressor().fit(xTrain,yTrain)\n",
    "    rf_train = pd.Series(rf.predict(xTrain))\n",
    "    rf_test = pd.Series(rf.predict(xTest))\n",
    "\n",
    "    #get data frames\n",
    "    lr_train_df = makeDF(lr_train, yTrain, name)\n",
    "    lr_test_df = makeDF(lr_test, yTest, name)\n",
    "\n",
    "    knn_train_df = makeDF(knn_train, yTrain, name)\n",
    "    knn_test_df = makeDF(knn_test, yTest, name)\n",
    "\n",
    "    ls_train_df = makeDF(ls_train, yTrain, name)\n",
    "    ls_test_df = makeDF(ls_test, yTest, name)\n",
    "\n",
    "    en_train_df = makeDF(en_train, yTrain, name)\n",
    "    en_test_df = makeDF(en_test, yTest, name)\n",
    "\n",
    "    rr_train_df = makeDF(rr_train, yTrain, name)\n",
    "    rr_test_df = makeDF(rr_test, yTest, name)\n",
    "\n",
    "    rf_train_df = makeDF(rf_train, yTrain, name)\n",
    "    rf_test_df = makeDF(rf_test, yTest, name)\n",
    "\n",
    "\n",
    "    #Get metrics for each df\n",
    "    lr_train_scores = calcScores(lr_train_df)\n",
    "    lr_train_scores.name = 'lr_train_scores'\n",
    "    lr_test_scores = calcScores(lr_test_df)\n",
    "    lr_test_scores.name = 'lr_test_scores'\n",
    "\n",
    "    knn_train_scores = calcScores(knn_train_df)\n",
    "    knn_train_scores.name = 'knn_train_scores'\n",
    "    knn_test_scores = calcScores(knn_test_df)\n",
    "    knn_test_scores.name = 'knn_test_scores'\n",
    "\n",
    "    ls_train_scores = calcScores(ls_train_df)\n",
    "    ls_train_scores.name = 'ls_train_scores'\n",
    "    ls_test_scores = calcScores(ls_test_df)\n",
    "    ls_test_scores.name = 'ls_test_scores'\n",
    "\n",
    "    en_train_scores = calcScores(en_train_df)\n",
    "    en_train_scores.name = 'en_train_scores'\n",
    "    en_test_scores = calcScores(en_test_df)\n",
    "    en_test_scores.name = 'en_test_scores'\n",
    "\n",
    "    rr_train_scores = calcScores(rr_train_df)\n",
    "    rr_train_scores.name = 'rr_train_scores'\n",
    "    rr_test_scores = calcScores(rr_test_df)\n",
    "    rr_test_scores.name = 'rr_test_scores'\n",
    "\n",
    "    rf_train_scores = calcScores(rf_train_df)\n",
    "    rf_train_scores.name = 'rf_train_scores'\n",
    "    rf_test_scores = calcScores(rf_test_df)\n",
    "    rf_test_scores.name = 'rf_test_scores'\n",
    "\n",
    "    #print all\n",
    "    print(pd.concat([lr_train_scores, lr_test_scores, \n",
    "                    knn_train_scores, knn_test_scores,\n",
    "                    ls_train_scores,  ls_test_scores,\n",
    "                    en_train_scores, en_test_scores,\n",
    "                    rr_train_scores, rr_test_scores,\n",
    "                    rf_train_scores,rf_test_scores],\n",
    "                    axis = 1))\n",
    "\n",
    "\n",
    "    #make a graph to show other models accuracy\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Closing Prices')\n",
    "    #plt.plot(train_data.index, yTrain, 'green', label='Train data')\n",
    "    plt.plot(test_data.index, yTest, 'blue', label='Test data')\n",
    "    plt.plot(test_data.index, fc_series, 'pink', label=' Arima pred data')\n",
    "    plt.plot(test_data.index, lr_test, 'yellow', label='LR pred data')\n",
    "    plt.plot(test_data.index, knn_test, 'green', label=' KNN pred data')\n",
    "    plt.plot(test_data.index, ls_test, 'red', label='LS pred data')\n",
    "    plt.plot(test_data.index, en_test, 'purple', label='EN pred data')\n",
    "    plt.plot(test_data.index, rr_test, 'black', label='RR pred data')\n",
    "    plt.plot(test_data.index, rf_test, 'orange', label='RF pred data')\n",
    "    plt.legend()\n",
    "\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "\n",
    "    # report performance\n",
    "    mse = mean_squared_error(test_data, fc)\n",
    "    #mse = metrics.mean_squared_error(test_data,fc)\n",
    "    print('MSE: '+str(mse))\n",
    "\n",
    "    mae = mean_absolute_error(test_data, fc)\n",
    "    #mae1 = metrics.mean_absolute_error(test_data,fc)\n",
    "    print('MAE: '+str(mae))\n",
    "\n",
    "    rmse = math.sqrt(mean_squared_error(test_data, fc))\n",
    "    print('RMSE: '+str(rmse))\n",
    "\n",
    "    mape = np.mean(np.abs(fc - test_data)/np.abs(test_data))\n",
    "    print('MAPE: '+str(mape))\n",
    "\n",
    "    medAe =  metrics.median_absolute_error(test_data,fc)\n",
    "    print('MedAE: ' + str(medAe))\n",
    "\n",
    "    rsq = metrics.r2_score(test_data, fc)\n",
    "    print('RSQ: '+ str(rsq))\n",
    "\n",
    "    explained_varience = metrics.explained_variance_score(test_data,fc)\n",
    "    print('Explained Varience: '+ str(explained_varience))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}